{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDw3-Qk6v192"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import torchvision.datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOdTOnGsd0Bu"
      },
      "source": [
        "# Introduction to Generative Adversarial Networks (GANs)\n",
        "\n",
        "In this notebook you will build a GAN from scratch, grossly following the DCGAN architecture.\n",
        "\n",
        "**Goals:**\n",
        "\n",
        "\n",
        "1.   Build a GAN arhictecture from scratch\n",
        "2.   Write the GAN loss function and train GAN from scratch\n",
        "3.   Have experience with some of the instability problems inherent with training GANs.\n",
        "4.   [Bonus] Extend the unconditional GAN into a conditional GAN.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uE0sMnZJdyhi"
      },
      "source": [
        "## 1. Getting Started\n",
        "\n",
        "We will work with the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. It contains 60,000 images size 28x28 of handwritten digits, from 0 to 9."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eljyV8Kqvlv1"
      },
      "source": [
        "## 1. Dataloading\n",
        "\n",
        "### 1.1 Define some hyperparameters and transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKoR9odDihPu",
        "outputId": "fac7c899-9651-4b6b-b56f-3b830a54944a"
      },
      "outputs": [],
      "source": [
        "batch_size = 128 # Images per batch\n",
        "\n",
        "# Resize to 32x32 for easier upsampling/downsampling\n",
        "mytransform = transforms.Compose([transforms.Resize(32),\n",
        "                                  transforms.ToTensor(),\n",
        "                                 transforms.Normalize((.5), (.5))]) # normalize between [-1, 1] with tanh activation\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(root='.', download=True, transform=mytransform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fyx7D9SjLRl"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset=mnist_train,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZjhs971tkUD"
      },
      "outputs": [],
      "source": [
        "# plot some images\n",
        "real_batch, real_labels = next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "rteQwXBRwIHK",
        "outputId": "e28515c2-4d55-4841-8449-b6036cffbfad"
      },
      "outputs": [],
      "source": [
        "plt.imshow(transforms.ToPILImage()(make_grid(real_batch)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HiqEo_eQe6mH"
      },
      "source": [
        "## 2. Model Architecture\n",
        "\n",
        "We will follow the general architecture of a DCGAN - or deep convolutional GAN. This [influential paper](https://arxiv.org/pdf/1511.06434v2.pdf) produced much of the foundation for modern GANs and how to train them.\n",
        "\n",
        "(GANs are notoriously **hard** to train, we will try to get a feeling why in this notebook.)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0fbtsFUJJvAo"
      },
      "source": [
        "Complete the function `get_upscaling_block`. Then, use the function to define our model defined as follows:\n",
        "\n",
        "Input: Random \"noise\" $z$ shaped `[n_batch, n_z]`\n",
        "\n",
        "Output: Generated image size `[n_batch, 1, 32, 32]` in range [-1, 1]\n",
        "\n",
        "1. Reshape z into `[n_batch, n_z, 1, 1]` to make it into an \"image\"\n",
        "2. First upscaling block $\\rightarrow$ `[n_batch, ngf*4, 4, 4]`\n",
        "3. Second upscaling block $\\rightarrow$ `[n_batch, ngf*2, 8, 8]`\n",
        "4. Third upscaling block $\\rightarrow$ `[n_batch, ngf, 16, 16]`\n",
        "5. Fourth (and last) upscaling block $\\rightarrow$ `[n_batch, 1, 32, 32]`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zSFr3RbOfOwP"
      },
      "source": [
        "## 2.1. The Generator\n",
        "\n",
        "\n",
        "\n",
        "The generator performs subsequent upsampling blocks, transforming a latent vector shaped [batch_size, latent_size] into an image (values in [-1, 1]).\n",
        "\n",
        "The generator block will consists of:\n",
        "- Transpose Convolution\n",
        "- Batch Norm\n",
        "- ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_upscaling_block(channels_in, channels_out, kernel, stride, padding, last_layer=False):\n",
        "    '''\n",
        "    Each transpose conv will be followed by BatchNorm and ReLU,\n",
        "    except the last block (which is only followed by tanh)\n",
        "    '''\n",
        "    layers = []\n",
        "    layers.append(nn.ConvTranspose2d(channels_in, channels_out, kernel, stride, padding, bias=False))\n",
        "    if not last_layer:\n",
        "        layers.append(nn.BatchNorm2d(channels_out))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "    else:\n",
        "        layers.append(nn.Tanh())\n",
        "    \n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdr4uE1tKhQt"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz, ngf, nchannels=1):\n",
        "        '''\n",
        "        nz: The latent size (100 in our case)\n",
        "        ngf: The channel-size before the last layer (32 our case)\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Reshape z [128,1000] into [n_batch, n_z, 1, 1]\n",
        "            nn.Unflatten(dim=1, unflattened_size=(nz, 1, 1)),\n",
        "            \n",
        "            # First upscaling block -> [n_batch, ngf*4, 4, 4]\n",
        "            get_upscaling_block(nz, ngf*4, kernel=4, stride=1, padding=0),\n",
        "            \n",
        "            # Second upscaling block -> [n_batch, ngf*2, 8, 8]\n",
        "            get_upscaling_block(ngf*4, ngf*2, kernel=4, stride=2, padding=1),\n",
        "            \n",
        "            # Third upscaling block -> [n_batch, ngf, 16, 16]\n",
        "            get_upscaling_block(ngf*2, ngf, kernel=4, stride=2, padding=1),\n",
        "            \n",
        "            # Fourth (and last) upscaling block -> [n_batch, 1, 32, 32]\n",
        "            get_upscaling_block(ngf, nchannels, kernel=4, stride=2, padding=1, last_layer=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # print(z.shape)\n",
        "        # Done in the nn.Sequential model with Unflatten \n",
        "        # x = z.unsqueeze(2).unsqueeze(2) # give spatial dimensions to z\n",
        "        # print(x.shape)\n",
        "        print(\"xshape\", z.shape)\n",
        "        return self.model(z)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sfyB-3_nSO_U"
      },
      "source": [
        "### 2.1.3. Sanity Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqIU5cq_Sb3R"
      },
      "outputs": [],
      "source": [
        "nz = 1000\n",
        "z = torch.randn(batch_size, nz)\n",
        "z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "G = Generator(nz=nz, ngf=16)\n",
        "G.model\n",
        "G(z).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoVw0MHUGMZY"
      },
      "outputs": [],
      "source": [
        "# G = Generator(nz=nz, ngf=16)\n",
        "# assert G(z).shape == (batch_size, 1, 32, 32)\n",
        "\n",
        "# G = Generator(nz=nz, ngf=16)\n",
        "# assert G(z).shape == (batch_size, 1, 32, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKlItI1IStKh"
      },
      "outputs": [],
      "source": [
        "# # visualize the output - at first it should just look like random noise!!\n",
        "# x_fake = G(z)\n",
        "# plt.imshow(transforms.ToPILImage()(make_grid(x_fake, nrow=8)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nW2OL64HUqdy"
      },
      "source": [
        "## 2.2. The discriminator\n",
        "\n",
        "The discriminator will be a mirror image of the generator."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4XhpkIjniEJI"
      },
      "source": [
        "The discriminator will also use a fully convolutional architecutre, with each block consisting of:\n",
        "- Conv layer\n",
        "- BatchNorm\n",
        "- ReLU\n",
        "\n",
        "Complete the function `get_downscaling_block` and then use it to define the following architecture for the discriminator:\n",
        "\n",
        "Input: Image shaped `[n_batch, 1, 32, 32]` (in [-1, 1])\n",
        "Output: Discriminator scores `[n_batch, 1]` in range [0, 1]\n",
        "\n",
        "1. First downscaling block $\\rightarrow$ `[n_batch, ndf, 16, 16]`\n",
        "2. Second downscaling block $\\rightarrow$ `[n_batch, ndf*2, 8, 8]`\n",
        "3. Third downscaling block $\\rightarrow$ `[n_batch, ndf*4, 4, 4]`\n",
        "4. Last downscaling block $\\rightarrow$ `[n_batch, 1]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_downscaling_block(channels_in, channels_out, kernel, stride, padding, use_batch_norm=True, is_last=False):\n",
        "    layers = []\n",
        "    layers.append(nn.Conv2d(channels_in, channels_out, kernel, stride, padding))\n",
        "    \n",
        "    if is_last:\n",
        "        layers.append(nn.Sigmoid())\n",
        "    elif not use_batch_norm:\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "    else:\n",
        "        layers.append(nn.BatchNorm2d(channels_out))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "    \n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHGPKZYShDW-"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ndf, nchannels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # Convolution (32 filters, kernel 4, stride 2, padding 1, no bias) + Batch Norm + LeakyReLU (α = 0.2)\n",
        "            get_downscaling_block(nchannels, ndf, kernel=4, stride=2, padding=1, use_batch_norm=True),\n",
        "            # Convolution (64 filters, kernel 4, stride 2, padding 1, no bias) + Batch Norm + LeakyReLU (α = 0.2)\n",
        "            get_downscaling_block(ndf, ndf*2, kernel=4, stride=2, padding=1, use_batch_norm=True),\n",
        "            # Convolution (128 filters, kernel 4, stride 2, padding 1, no bias) + Batch Norm + LeakyReLU (α = 0.2)\n",
        "            get_downscaling_block(ndf*2, ndf*4, kernel=4, stride=2, padding=1, use_batch_norm=True),\n",
        "            # Convolution (1 filter, kernel 4, stride 1, padding 0, no bias) + Sigmoid activation\n",
        "            get_downscaling_block(ndf*4, 1, kernel=4, stride=1, padding=0, is_last=True, use_batch_norm=False),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"discr x forward\",x.shape)\n",
        "        return self.model(x).squeeze(1).squeeze(1) # remove spatial dimensions --> TODO: it can be done with Unflatten as in the generator"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OGzrUeMJkWCR"
      },
      "source": [
        "### 2.2.3 Sanity Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UQyIgSykUgU"
      },
      "outputs": [],
      "source": [
        "real_batch, real_labels = next(iter(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmtSAI81kuhS"
      },
      "outputs": [],
      "source": [
        "D = Discriminator(ndf=32, nchannels=1)\n",
        "print(\"d real shape\", D(real_batch).shape)\n",
        "print(real_batch.shape)\n",
        "assert D(real_batch).shape == (real_batch.shape[0], 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "skKGHErnmo70"
      },
      "source": [
        "# 3. Training\n",
        "\n",
        "## 3.1 Loss\n",
        "The essential thing to remember is that the logistic and the non-saturating logistic GAN losses can be written exclusively using the [binary cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html). Our GAN loss will be defined ONLY using the following criterion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pWhEANEmoPM"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss() # we will build off of this to make our final GAN loss!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "umYgNJN2rl2Y"
      },
      "source": [
        "## 3.2 Helper functions\n",
        "\n",
        "We will need a few helper functions.\n",
        "1. First, we need to continuously sample z from a Gaussian distribution.\n",
        "2. Secondly, we need to make our \"ground-truth\" labels when using the BCE loss. This should output vectors of either 0s or 1s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9XYCOrbngJF"
      },
      "outputs": [],
      "source": [
        "def sample_z(batch_size, nz):\n",
        "    return torch.randn(batch_size, nz, device=device)\n",
        "\n",
        "# this is for the real ground-truth label (real labels)\n",
        "def get_labels_one(batch_size):\n",
        "    r = torch.ones(batch_size, 1)\n",
        "    return r.to(device)\n",
        "\n",
        "# this is for the generated ground-truth label (fake labels)\n",
        "def get_labels_zero(batch_size):\n",
        "    r = torch.zeros(batch_size, 1)\n",
        "    return r.to(device)\n",
        "\n",
        "\n",
        "# To initialize the weights of a GAN, the DCGAN paper found that best results are obtained\n",
        "# with Gaussian initialization with mean=0; std=0.02\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "# for visualization\n",
        "to_pil = transforms.ToPILImage()\n",
        "renorm = transforms.Normalize((-1.), (2.))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7rxgJpPynsNz"
      },
      "source": [
        "## 3.3 Creating the optimizers and hyperparameters\n",
        "\n",
        "The original DCGAN paper shows that Adam works well in the generator and the discriminator with a learning rate of 0.0002 and Beta1 = 0.5.\n",
        "\n",
        "Define your optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDTs6bGfnrP8"
      },
      "outputs": [],
      "source": [
        "nz = 100\n",
        "ngf = 32\n",
        "ndf = 32\n",
        "\n",
        "nchannels= 1\n",
        "lr_d = 0.0002\n",
        "lr_g = 0.0005\n",
        "beta1= 0.5\n",
        "display_freq = 200\n",
        "\n",
        "netD = Discriminator(ndf, nchannels).to(device)\n",
        "netG = Generator(nz, ngf).to(device)\n",
        "\n",
        "netD.apply(weights_init)\n",
        "netG.apply(weights_init)\n",
        "\n",
        "g_opt = torch.optim.Adam(netG.parameters(), lr=lr_g, betas=(beta1, 0.999))\n",
        "d_opt = torch.optim.Adam(netD.parameters(), lr=lr_d, betas=(beta1, 0.999))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kaE6o8Z2uBza"
      },
      "source": [
        "## 3.3 Training!\n",
        "\n",
        "Now for the fun part, training!\n",
        "Training a GAN consists in making an update to the discriminator, then the generator.\n",
        "Training a GAN requires BABYSITTING!! Remember that many things can go wrong when training a GAN:\n",
        "- The discriminator is too strong for the generator - the generator cannot improve.\n",
        "- The generator easily fools the discriminator - cannot learn.\n",
        "- Mode collapse - generator is not capable of generating diverse images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy03mvvmuBMo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "nb_epochs = 5\n",
        "\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "\n",
        "j = 0\n",
        "\n",
        "z_test = sample_z(64, nz) # we generate the noise only once for testing\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "\n",
        "    # train\n",
        "    pbar = tqdm(enumerate(dataloader))\n",
        "    for i, batch in pbar:\n",
        "\n",
        "        # 1. construct a batch of real samples from the training set (sample a z vector)\n",
        "        im, _ = batch # we don't care about the label for unconditional generation\n",
        "        im = im.to(device) # real image\n",
        "        cur_batch_size = im.shape[0] # batch size\n",
        "        z = sample_z(cur_batch_size, nz)\n",
        "        # label_real = torch.full((cur_batch_size,), 1., dtype=torch.float, device=device)\n",
        "        label_real= get_labels_one(cur_batch_size).view(-1)\n",
        "\n",
        "        # 2. forward pass through D (=Classify real image with D)\n",
        "        yhat_real = netD(im).view(-1) # the size -1 is inferred from other dimensions\n",
        "\n",
        "        # 3. forward pass through G (=Generate fake image batch with G)\n",
        "        y_fake = netG(z)\n",
        "        # label_fake=label_real.fill_(0.)\n",
        "        label_fake= get_labels_zero(cur_batch_size).view(-1)\n",
        "\n",
        "        # 4. Classify fake image with D\n",
        "        print(\"y fake\", y_fake.shape )\n",
        "        yhat_fake = netD(y_fake.detach()).view(-1)\n",
        "\n",
        "        ### Discriminator\n",
        "        d_loss = criterion(yhat_real,label_real) + criterion(yhat_fake,label_fake) # TODO check loss\n",
        "        d_opt.zero_grad()\n",
        "        d_loss.backward(retain_graph=True) # we need to retain graph=True to be able to calculate the gradient in the g backprop\n",
        "        d_opt.step()\n",
        "\n",
        "\n",
        "        ### Generator\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        yhat_fake = netD(y_fake).view(-1)\n",
        "        g_loss = criterion(yhat_fake, label_real) # fake labels are real for generator cost\n",
        "        g_opt.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_opt.step()\n",
        "\n",
        "\n",
        "        # Save Metrics\n",
        "\n",
        "        d_losses.append(d_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "        avg_real_score = yhat_real.mean().item()\n",
        "        avg_fake_score = yhat_fake.mean().item()\n",
        "\n",
        "\n",
        "\n",
        "        pbar.set_description(f\"it: {j}; g_loss: {g_loss}; d_loss: {d_loss}; avg_real_score: {avg_real_score}; avg_fake_score: {avg_fake_score}\")\n",
        "        if i % display_freq == 0:\n",
        "            \n",
        "            print(z_test.shape)\n",
        "            print(netG.model)\n",
        "            fake_im = netG(z_test)\n",
        "\n",
        "            un_norm = renorm(fake_im) # for visualization\n",
        "\n",
        "            grid = torchvision.utils.make_grid(un_norm, nrow=8)\n",
        "            pil_grid = to_pil(grid)\n",
        "\n",
        "            print(\"generated images\")\n",
        "            plt.imshow(pil_grid)\n",
        "            plt.show()\n",
        "\n",
        "            plt.plot(range(len(g_losses)), g_losses, label='g loss')\n",
        "            plt.plot(range(len(g_losses)), d_losses, label='d loss')\n",
        "\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        j += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interpolate(num_steps=20, number_of_examples=15, netG=netG, latent_size=100):\n",
        "    assert num_steps > 1, \"Minimum number of steps should be 2\"\n",
        "\n",
        "    z1 = torch.randn((number_of_examples, latent_size)) \n",
        "    z2 = torch.randn((number_of_examples, latent_size))\n",
        "    alphas = torch.linspace(0, 1, num_steps)\n",
        "\n",
        "    vectors_to_concat = []\n",
        "\n",
        "    for b in range(number_of_examples):\n",
        "        for a in alphas:\n",
        "            vectors_to_concat.append(a * z2[b] + (1.0 - a) * z1[b])\n",
        "\n",
        "    zs = torch.cat(vectors_to_concat).view(-1, latent_size).to(device)\n",
        "\n",
        "    netG.eval()\n",
        "\n",
        "    fake_ims = netG(zs)\n",
        "\n",
        "    print(fake_ims.shape)\n",
        "\n",
        "    un_norm = renorm(fake_ims) # for visualization\n",
        "    grid = torchvision.utils.make_grid(un_norm, nrow=num_steps)\n",
        "    pil_grid = to_pil(grid)\n",
        "\n",
        "    plt.imshow(pil_grid)\n",
        "    plt.show()\n",
        "    \n",
        "interpolate()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sp2kL5GZxC4K"
      },
      "source": [
        "# 4. Conditional GANs [BONUS]\n",
        "\n",
        "A simple way to improve training performance and obtain control of the generation is to provide extra information into the Generator and the Discriminator, known as **Conditional GANs**. In this case, we will provide the class label (digit number of MNIST) into both the generator and the discriminator. This will help both of the networks."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XeUMj-oKd7yI"
      },
      "source": [
        "## 4.1 Conditional Generator and Discriminator\n",
        "\n",
        "Complete the ConditionalDiscriminator and ConditionalGenerator classes using your GAN building blocks (`get_upsampling_block` and `get_downsampling_block` functions).\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fYr7d0GTf6in"
      },
      "source": [
        "### 4.1.1 Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7We6x1vZfXFi"
      },
      "outputs": [],
      "source": [
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, nz, nc, ngf, nchannels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Transpose Convolution (256 filters, kernel 4, stride 1, padding 0, no bias) + Batch Norm + ReLU\n",
        "        self.upscaling_z = get_upscaling_block(nz, ngf*16, kernel=4, stride=1, padding=0) # branch 1\n",
        "        self.upscaling_c = get_upscaling_block(nc, ngf*16, kernel=4, stride=1, padding=0) # branch 2\n",
        "\n",
        "        self.rest_model = nn.Sequential(\n",
        "            # Concatenate branches 1 and 2 (256 + 256 channels)\n",
        "            get_upscaling_block(ngf*16 + ngf*16, ngf*16, kernel=4, stride=2, padding=1),\n",
        "            # Transpose Convolution (128 filters, kernel 4, stride 2, padding 1, no bias) + Batch Norm + ReLU\n",
        "            get_upscaling_block(ngf*16, ngf*8, kernel=4, stride=2, padding=1),\n",
        "            # Transpose Convolution (1 filter, kernel 4, stride 2, padding 1, no bias)\n",
        "            get_upscaling_block(ngf*8, nchannels, kernel=4, stride=2, padding=1, last_layer=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.unsqueeze(2).unsqueeze(2)\n",
        "        y = y.unsqueeze(2).unsqueeze(2)\n",
        "        \n",
        "        x = self.upscaling_z(x)\n",
        "        y = self.upscaling_c(y)\n",
        "        \n",
        "        combined = torch.cat([x, y], 1)\n",
        "        return self.rest_model(combined)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bNgw9wbff-Dr"
      },
      "source": [
        "### 4.1.2 Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3S02w7Kfz1j"
      },
      "outputs": [],
      "source": [
        "# Discriminator\n",
        "\n",
        "'''\n",
        "The conditional discriminator needs the label information as well as the latent vector. We will combine the latent vector and the class information in the following way:\n",
        "\n",
        "- The class information for the discriminator will be represented as a one-hot vector sized `[batch_size, 10]` (since there are 10 classes in MNIST)\n",
        "- The latent vector for the generator will still be sized `[batch_size, nz]`\n",
        "\n",
        "1. Transform both of these modalities into 'images' (by adding dimensions)\n",
        "2. Like before, apply the first upscaling block to both of these 'images'. We will now have 2 separate blocks sized\n",
        "\n",
        "'''\n",
        "\n",
        "class ConditionalDiscriminator(nn.Module):\n",
        "    def __init__(self, ndf, nc, nchannels=1):\n",
        "        super().__init__()\n",
        "        self.downscale_x = get_downscaling_block(nchannels, ndf*2, 4, 2, 1, use_batch_norm=False)\n",
        "        self.downscale_y = get_downscaling_block(nc, ndf*2, 4, 2, 1, use_batch_norm=False)\n",
        "\n",
        "\n",
        "        self.rest = nn.Sequential(\n",
        "            get_downscaling_block(ndf*2 + ndf*2, ndf*8, kernel=4, stride=2, padding=1, use_batch_norm=True),\n",
        "            get_downscaling_block(ndf*8, ndf*16, kernel=4, stride=2, padding=1, use_batch_norm=True),\n",
        "            get_downscaling_block(ndf*16, 1, kernel=4, stride=1, padding=0, is_last=True, use_batch_norm=False),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #Expansion of y into a tensor sized 10 × 32 × 32\n",
        "        y = y.unsqueeze(2).unsqueeze(2).expand(-1, -1, x.shape[2], x.shape[3])\n",
        "        x = self.downscale_x(x)\n",
        "        y = self.downscale_y(y)\n",
        "        # Concatenate the two feature maps along the channel dimension\n",
        "        combined = torch.cat([x, y], dim=1)\n",
        "\n",
        "        return self.rest(combined).squeeze(1).squeeze(1) # remove spatial dimensions\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RA7nJF9SgDgt"
      },
      "source": [
        "### 4.1.3 Sanity Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1TbRNIqffip"
      },
      "outputs": [],
      "source": [
        "# CONDITIONAL GENERATOR SANITY CHECKING\n",
        "\n",
        "z = torch.randn(batch_size, nz)\n",
        "y = F.one_hot(real_labels).float()\n",
        "\n",
        "cG = ConditionalGenerator(nz=nz, ngf=16, nc=10)\n",
        "assert cG(z,y).shape == (batch_size, 1, 32, 32)\n",
        "\n",
        "x_fake = cG(z, y)\n",
        "plt.imshow(transforms.ToPILImage()(make_grid(x_fake, nrow=8)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlUn9fVUftdx"
      },
      "outputs": [],
      "source": [
        "# Conditional Discriminator Sanity Checking\n",
        "\n",
        "cD = ConditionalDiscriminator(ndf=32, nc=10, nchannels=1)\n",
        "print(cD.rest)\n",
        "assert cD(real_batch, F.one_hot(real_labels).float()).shape == (real_batch.shape[0], 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EHFUef5bfVXD"
      },
      "source": [
        "## 4.2 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uY50uzNm9Bi"
      },
      "outputs": [],
      "source": [
        "nz = 100\n",
        "ndf = 32\n",
        "ngf = 32\n",
        "nchannels= 1\n",
        "lr_d = 0.0002\n",
        "lr_g = 0.0005\n",
        "beta1= 0.5\n",
        "display_freq = 200\n",
        "\n",
        "nc= 10\n",
        "\n",
        "netD = ConditionalDiscriminator(ndf, nc, nchannels=1).to(device)\n",
        "netG = ConditionalGenerator(nz, nc, ngf).to(device)\n",
        "\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "g_opt = torch.optim.Adam(netG.parameters(), lr=lr_g, betas=(beta1, 0.999))\n",
        "d_opt = torch.optim.Adam(netD.parameters(), lr=lr_d, betas=(beta1, 0.999))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqiuCsSSm9Bi"
      },
      "outputs": [],
      "source": [
        "nb_epochs = 5\n",
        "\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "\n",
        "j = 0\n",
        "\n",
        "z_test = sample_z(100, nz)  # we generate the noise only once for testing\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    pbar = tqdm(enumerate(dataloader))\n",
        "    for i, batch in pbar:\n",
        "        im, labels = batch\n",
        "        im = im.to(device)\n",
        "        y = F.one_hot(labels).float().to(device)\n",
        "        cur_batch_size = im.shape[0]\n",
        "        z = sample_z(cur_batch_size, nz)\n",
        "        # label_real = torch.full((cur_batch_size,), 1., dtype=torch.float, device=device)\n",
        "        label_real= get_labels_one(cur_batch_size).view(-1)\n",
        "\n",
        "        # 2. forward pass through D (=Classify real image with D)\n",
        "        yhat_real = netD(im,y).view(-1) # the size -1 is inferred from other dimensions\n",
        "\n",
        "        # 3. forward pass through G (=Generate fake image batch with G)\n",
        "        y_fake = netG(z, y)\n",
        "        # label_fake=label_real.fill_(0.)\n",
        "        label_fake= get_labels_zero(cur_batch_size).view(-1)\n",
        "\n",
        "        # 4. Classify fake image with D\n",
        "        yhat_fake = netD(y_fake.detach(), y).view(-1)\n",
        "\n",
        "        ### Discriminator\n",
        "        d_loss = criterion(yhat_real,label_real) + criterion(yhat_fake,label_fake) # TODO check loss\n",
        "        d_opt.zero_grad()\n",
        "        d_loss.backward(retain_graph=True) # we need to retain graph=True to be able to calculate the gradient in the g backprop\n",
        "        d_opt.step()\n",
        "\n",
        "\n",
        "        ### Generator\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        yhat_fake = netD(y_fake, y).view(-1)\n",
        "        g_loss = criterion(yhat_fake, label_real) # fake labels are real for generator cost\n",
        "        g_opt.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_opt.step()\n",
        "\n",
        "        # Save Metrics\n",
        "        d_losses.append(d_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "        avg_real_score = yhat_real.mean().item()\n",
        "        avg_fake_score = yhat_fake.mean().item()\n",
        "\n",
        "        pbar.set_description(f\"it: {j}; g_loss: {g_loss}; d_loss: {d_loss}; avg_real_score: {avg_real_score}; avg_fake_score: {avg_fake_score}\")\n",
        "\n",
        "        if i % display_freq == 0:\n",
        "            labels = torch.arange(0, 10).expand(size=(10, 10)).flatten().to(device)\n",
        "            y = F.one_hot(labels).float().to(device)\n",
        "            fake_im = netG(z_test, y)\n",
        "\n",
        "            un_norm = renorm(fake_im) # for visualization\n",
        "\n",
        "            grid = torchvision.utils.make_grid(un_norm, nrow=10)\n",
        "            pil_grid = to_pil(grid)\n",
        "\n",
        "            plt.imshow(pil_grid)\n",
        "            plt.show()\n",
        "\n",
        "            plt.plot(range(len(g_losses)), g_losses, label='g loss')\n",
        "            plt.plot(range(len(g_losses)), d_losses, label='d loss')\n",
        "\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        j += 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MTxWRkC0m9Bj"
      },
      "source": [
        "## 4.2 Testing\n",
        "Visualization of the impact of z on generation. All digits in the same column have the same noise vector z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNxBF1zpm9Bj"
      },
      "outputs": [],
      "source": [
        "n_ex = 15\n",
        "n_classes = 10\n",
        "\n",
        "z = sample_z(n_ex, nz).repeat(n_classes, 1) #.expand(10, nz)\n",
        "\n",
        "netG.eval()\n",
        "\n",
        "labels = torch.arange(n_classes).unsqueeze(0).reshape(-1, 1).repeat(1, n_ex).flatten().to(device)\n",
        "\n",
        "ys = F.one_hot(labels).float()\n",
        "fake_ims = netG(z, ys)\n",
        "\n",
        "un_norm= renorm(fake_ims) # for visualization\n",
        "grid = torchvision.utils.make_grid(un_norm, nrow=n_ex)\n",
        "pil_grid = to_pil(grid)\n",
        "\n",
        "plt.imshow(pil_grid)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlW5zajDm9Bj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
